package com.mlengine.service;

import com.mlengine.model.dto.ExplainabilityDTO;
import com.mlengine.model.entity.Model;
import com.mlengine.repository.ModelRepository;
import lombok.RequiredArgsConstructor;
import lombok.extern.slf4j.Slf4j;
import org.springframework.stereotype.Service;

import java.util.*;
import java.util.stream.Collectors;

/**
 * Service for Model Explainability (SHAP, LIME, PDP, What-If).
 * Provides interpretability features for React UI.
 */
@Slf4j
@Service
@RequiredArgsConstructor
public class ExplainabilityService {

    private final ModelRepository modelRepository;

    // Sample feature names for demo data
    private static final String[] FEATURE_NAMES = {
            "transaction_amount", "account_age", "num_transactions",
            "avg_balance", "credit_score", "num_products",
            "customer_tenure", "last_activity_days", "payment_history",
            "income_bracket", "age", "employment_years"
    };

    // ========== SHAP ==========

    /**
     * Get SHAP global feature importance.
     * Matches React UI SHAP Global Feature Importance view.
     */
    public ExplainabilityDTO.ShapGlobal getShapGlobal(String modelId) {
        Model model = getModelOrThrow(modelId);

        List<ExplainabilityDTO.ShapFeature> features = new ArrayList<>();
        double[] importances = {0.28, 0.19, 0.14, 0.11, 0.09, 0.07, 0.05, 0.03, 0.02, 0.02};

        for (int i = 0; i < Math.min(FEATURE_NAMES.length, importances.length); i++) {
            String direction = i % 3 == 0 ? "positive" : (i % 3 == 1 ? "negative" : "mixed");
            double posImpact = direction.equals("positive") ? importances[i] * 0.7 : importances[i] * 0.3;
            double negImpact = direction.equals("negative") ? importances[i] * 0.7 : importances[i] * 0.3;

            features.add(ExplainabilityDTO.ShapFeature.builder()
                    .feature(FEATURE_NAMES[i])
                    .importance(importances[i])
                    .importanceLabel(String.format("%.2f", importances[i]))
                    .positiveImpact(posImpact)
                    .negativeImpact(negImpact)
                    .direction(direction)
                    .rank(i + 1)
                    .build());
        }

        return ExplainabilityDTO.ShapGlobal.builder()
                .modelId(modelId)
                .features(features)
                .explanation("Feature importance computed using SHAP (SHapley Additive exPlanations). " +
                        "Higher values indicate greater impact on model predictions.")
                .build();
    }

    /**
     * Get SHAP summary plot data.
     * For beeswarm/violin visualization.
     */
    public ExplainabilityDTO.ShapSummary getShapSummary(String modelId, int sampleCount) {
        Model model = getModelOrThrow(modelId);

        int numFeatures = 10;
        int numSamples = Math.min(sampleCount, 100);

        List<String> featureNames = Arrays.asList(FEATURE_NAMES).subList(0, numFeatures);
        List<List<Double>> shapValues = new ArrayList<>();
        List<List<Double>> featureValues = new ArrayList<>();

        Random random = new Random(42);

        for (int f = 0; f < numFeatures; f++) {
            List<Double> shapForFeature = new ArrayList<>();
            List<Double> valuesForFeature = new ArrayList<>();

            double importance = 0.3 - (f * 0.025);  // Decreasing importance

            for (int s = 0; s < numSamples; s++) {
                // Generate correlated SHAP and feature values
                double featureVal = random.nextGaussian();
                double shapVal = featureVal * importance + random.nextGaussian() * 0.1;

                shapForFeature.add(shapVal);
                valuesForFeature.add(featureVal);
            }

            shapValues.add(shapForFeature);
            featureValues.add(valuesForFeature);
        }

        return ExplainabilityDTO.ShapSummary.builder()
                .modelId(modelId)
                .features(featureNames)
                .shapValues(shapValues)
                .featureValues(featureValues)
                .sampleCount(numSamples)
                .build();
    }

    /**
     * Get SHAP local explanation for individual prediction.
     * Matches React UI Individual Prediction Explanation.
     */
    public ExplainabilityDTO.ShapLocal getShapLocal(String modelId, ExplainabilityDTO.ShapLocalRequest request) {
        Model model = getModelOrThrow(modelId);

        Map<String, Object> features = request.getFeatures();
        int topK = request.getTopK() != null ? request.getTopK() : 10;

        // Base value (average model output)
        double baseValue = 0.35;
        double predictedValue = 0.78;

        List<ExplainabilityDTO.ShapContribution> contributions = new ArrayList<>();

        // Generate contributions based on input features
        double remainingContribution = predictedValue - baseValue;
        List<String> featureKeys = new ArrayList<>(features.keySet());

        for (int i = 0; i < Math.min(featureKeys.size(), topK); i++) {
            String feature = featureKeys.get(i);
            Object value = features.get(feature);

            // Assign contribution (decreasing)
            double shapValue = remainingContribution * (0.4 - i * 0.05);
            if (i % 2 == 1) shapValue = -shapValue * 0.5;  // Some negative

            remainingContribution -= Math.abs(shapValue);

            String direction = shapValue > 0 ? "positive" : "negative";
            String impact = shapValue > 0 
                    ? "High value increases prediction" 
                    : "Low value decreases prediction";

            contributions.add(ExplainabilityDTO.ShapContribution.builder()
                    .feature(feature)
                    .featureValue(value)
                    .shapValue(shapValue)
                    .shapLabel(String.format("%+.2f", shapValue))
                    .direction(direction)
                    .impact(impact)
                    .build());
        }

        // Sort by absolute SHAP value
        contributions.sort((a, b) -> Double.compare(
                Math.abs(b.getShapValue()), Math.abs(a.getShapValue())));

        return ExplainabilityDTO.ShapLocal.builder()
                .modelId(modelId)
                .baseValue(baseValue)
                .predictedValue(predictedValue)
                .predictedClass("Positive")
                .probability(predictedValue)
                .contributions(contributions)
                .explanation(generateLocalExplanation(contributions, predictedValue))
                .build();
    }

    // ========== LIME ==========

    /**
     * Get LIME explanation for individual prediction.
     * Matches React UI LIME Explanation section.
     */
    public ExplainabilityDTO.LimeExplanation getLimeExplanation(
            String modelId, ExplainabilityDTO.LimeRequest request) {
        
        Model model = getModelOrThrow(modelId);

        Map<String, Object> features = request.getFeatures();
        int numFeatures = request.getNumFeatures() != null ? request.getNumFeatures() : 10;

        List<ExplainabilityDTO.LimeFeature> limeFeatures = new ArrayList<>();

        // Generate LIME-style explanations with conditions
        String[] conditions = {
                "> 50000", "<= 35", "> 5", "= High", "<= 2",
                "> 700", "= Yes", "<= 30", "> 3", "= Premium"
        };

        double[] weights = {0.25, -0.18, 0.14, 0.12, -0.09, 0.08, 0.06, -0.05, 0.04, 0.03};

        List<String> featureKeys = new ArrayList<>(features.keySet());
        for (int i = 0; i < Math.min(featureKeys.size(), numFeatures); i++) {
            String feature = featureKeys.get(i);
            String condition = feature + " " + conditions[i % conditions.length];
            double weight = weights[i % weights.length];

            limeFeatures.add(ExplainabilityDTO.LimeFeature.builder()
                    .feature(feature)
                    .condition(condition)
                    .weight(weight)
                    .weightLabel(String.format("%+.2f", weight))
                    .direction(weight > 0 ? "positive" : "negative")
                    .build());
        }

        // Sort by absolute weight
        limeFeatures.sort((a, b) -> Double.compare(
                Math.abs(b.getWeight()), Math.abs(a.getWeight())));

        return ExplainabilityDTO.LimeExplanation.builder()
                .modelId(modelId)
                .predictedProbability(0.78)
                .predictedClass("Positive")
                .features(limeFeatures)
                .intercept(0.35)
                .localPrediction(0.78)
                .r2Score(0.92)
                .explanation("LIME approximates the model locally using a linear model. " +
                        "The RÂ² score of 0.92 indicates a good local fit.")
                .build();
    }

    // ========== PDP & ICE ==========

    /**
     * Get Partial Dependence Plot data.
     * Matches React UI PDP visualization.
     */
    public ExplainabilityDTO.PartialDependence getPartialDependence(String modelId, String feature) {
        Model model = getModelOrThrow(modelId);

        // Generate PDP data
        int numPoints = 50;
        List<Object> featureValues = new ArrayList<>();
        List<Double> pdpValues = new ArrayList<>();

        double min = 0, max = 100;
        double step = (max - min) / numPoints;

        for (int i = 0; i <= numPoints; i++) {
            double x = min + i * step;
            featureValues.add(x);

            // Generate S-curve or linear relationship
            double y = 0.3 + 0.4 / (1 + Math.exp(-(x - 50) / 15));
            pdpValues.add(y);
        }

        String interpretation = pdpValues.get(pdpValues.size() - 1) > pdpValues.get(0)
                ? "Higher values of " + feature + " increase the prediction probability"
                : "Higher values of " + feature + " decrease the prediction probability";

        return ExplainabilityDTO.PartialDependence.builder()
                .modelId(modelId)
                .feature(feature)
                .featureType("numeric")
                .featureValues(featureValues)
                .pdpValues(pdpValues)
                .featureMin(min)
                .featureMax(max)
                .featureMean(50.0)
                .interpretation(interpretation)
                .build();
    }

    /**
     * Get ICE (Individual Conditional Expectation) plot data.
     */
    public ExplainabilityDTO.IcePlot getIcePlot(String modelId, String feature, int sampleCount) {
        Model model = getModelOrThrow(modelId);

        int numPoints = 50;
        int numSamples = Math.min(sampleCount, 50);

        List<Object> featureValues = new ArrayList<>();
        List<List<Double>> iceLines = new ArrayList<>();
        List<Double> pdpLine = new ArrayList<>();

        Random random = new Random(42);

        // Generate feature values (x-axis)
        for (int i = 0; i <= numPoints; i++) {
            featureValues.add(i * 2.0);  // 0 to 100
        }

        // Generate ICE lines for each sample
        for (int s = 0; s < numSamples; s++) {
            List<Double> iceLine = new ArrayList<>();
            double baseOffset = random.nextGaussian() * 0.1;
            double slope = 0.004 + random.nextGaussian() * 0.001;

            for (int i = 0; i <= numPoints; i++) {
                double x = i * 2.0;
                double y = 0.3 + baseOffset + slope * x + random.nextGaussian() * 0.02;
                iceLine.add(Math.max(0, Math.min(1, y)));
            }
            iceLines.add(iceLine);
        }

        // Calculate PDP (average of ICE lines)
        for (int i = 0; i <= numPoints; i++) {
            final int idx = i;
            double avg = iceLines.stream()
                    .mapToDouble(line -> line.get(idx))
                    .average()
                    .orElse(0.5);
            pdpLine.add(avg);
        }

        return ExplainabilityDTO.IcePlot.builder()
                .modelId(modelId)
                .feature(feature)
                .featureValues(featureValues)
                .iceLines(iceLines)
                .pdpLine(pdpLine)
                .sampleCount(numSamples)
                .build();
    }

    // ========== WHAT-IF ANALYSIS ==========

    /**
     * Perform What-If analysis.
     * Matches React UI What-If Analysis section.
     */
    public ExplainabilityDTO.WhatIfResponse analyzeWhatIf(
            String modelId, ExplainabilityDTO.WhatIfRequest request) {
        
        Model model = getModelOrThrow(modelId);

        Map<String, Object> original = request.getOriginalFeatures();
        Map<String, Object> modified = request.getModifiedFeatures();

        // Simulate predictions
        double originalProb = 0.72;
        double newProb = 0.45;
        double change = newProb - originalProb;

        // Analyze feature changes
        List<ExplainabilityDTO.FeatureChange> featureChanges = new ArrayList<>();

        for (String feature : modified.keySet()) {
            if (original.containsKey(feature)) {
                Object origVal = original.get(feature);
                Object newVal = modified.get(feature);

                if (!origVal.equals(newVal)) {
                    double impact = (Math.random() - 0.5) * 0.2;  // Random impact for demo

                    featureChanges.add(ExplainabilityDTO.FeatureChange.builder()
                            .feature(feature)
                            .originalValue(origVal)
                            .newValue(newVal)
                            .impact(impact)
                            .impactLabel(String.format("%+.1f%%", impact * 100))
                            .build());
                }
            }
        }

        // Sort by impact
        featureChanges.sort((a, b) -> Double.compare(
                Math.abs(b.getImpact()), Math.abs(a.getImpact())));

        // Generate recommendations
        List<ExplainabilityDTO.Recommendation> recommendations = generateRecommendations(
                original, originalProb, model);

        String changeDirection = change > 0 ? "increased" : "decreased";
        String explanation = String.format(
                "Based on the changes made, the prediction probability %s from %.1f%% to %.1f%%. " +
                "The most impactful change was %s.",
                changeDirection,
                originalProb * 100,
                newProb * 100,
                featureChanges.isEmpty() ? "none" : featureChanges.get(0).getFeature()
        );

        return ExplainabilityDTO.WhatIfResponse.builder()
                .modelId(modelId)
                .originalProbability(originalProb)
                .originalClass(originalProb > 0.5 ? "Positive" : "Negative")
                .originalRiskLevel(getRiskLevel(originalProb))
                .newProbability(newProb)
                .newClass(newProb > 0.5 ? "Positive" : "Negative")
                .newRiskLevel(getRiskLevel(newProb))
                .probabilityChange(change)
                .probabilityChangeLabel(String.format("%+.1f%%", change * 100))
                .changeDirection(changeDirection)
                .featureChanges(featureChanges)
                .recommendations(recommendations)
                .explanation(explanation)
                .build();
    }

    /**
     * Get counterfactual explanation.
     */
    public ExplainabilityDTO.Counterfactual getCounterfactual(
            String modelId, Map<String, Object> features, String targetClass) {
        
        Model model = getModelOrThrow(modelId);

        double originalProb = 0.72;
        double counterfactualProb = 0.35;

        // Generate required changes
        List<ExplainabilityDTO.FeatureChange> changes = new ArrayList<>();

        changes.add(ExplainabilityDTO.FeatureChange.builder()
                .feature("credit_score")
                .originalValue(650)
                .newValue(720)
                .impact(-0.15)
                .impactLabel("-15%")
                .build());

        changes.add(ExplainabilityDTO.FeatureChange.builder()
                .feature("account_age")
                .originalValue(12)
                .newValue(24)
                .impact(-0.12)
                .impactLabel("-12%")
                .build());

        changes.add(ExplainabilityDTO.FeatureChange.builder()
                .feature("num_transactions")
                .originalValue(5)
                .newValue(15)
                .impact(-0.10)
                .impactLabel("-10%")
                .build());

        return ExplainabilityDTO.Counterfactual.builder()
                .modelId(modelId)
                .originalClass("Positive")
                .targetClass(targetClass)
                .originalProbability(originalProb)
                .counterfactualProbability(counterfactualProb)
                .requiredChanges(changes)
                .explanation("To change the prediction from Positive to " + targetClass + 
                        ", the following changes would be required.")
                .build();
    }

    // ========== HELPER METHODS ==========

    private Model getModelOrThrow(String modelId) {
        return modelRepository.findById(modelId)
                .orElseThrow(() -> new IllegalArgumentException("Model not found: " + modelId));
    }

    private String generateLocalExplanation(List<ExplainabilityDTO.ShapContribution> contributions, double prediction) {
        if (contributions.isEmpty()) {
            return "No significant feature contributions identified.";
        }

        StringBuilder sb = new StringBuilder();
        sb.append(String.format("The model predicts %.1f%% probability. ", prediction * 100));

        ExplainabilityDTO.ShapContribution top = contributions.get(0);
        if (top.getShapValue() > 0) {
            sb.append(String.format("The main factor increasing the prediction is %s (value: %s, contribution: %s). ",
                    top.getFeature(), top.getFeatureValue(), top.getShapLabel()));
        } else {
            sb.append(String.format("The main factor decreasing the prediction is %s (value: %s, contribution: %s). ",
                    top.getFeature(), top.getFeatureValue(), top.getShapLabel()));
        }

        return sb.toString();
    }

    private List<ExplainabilityDTO.Recommendation> generateRecommendations(
            Map<String, Object> features, double currentProb, Model model) {
        
        List<ExplainabilityDTO.Recommendation> recommendations = new ArrayList<>();

        // Generate contextual recommendations
        recommendations.add(ExplainabilityDTO.Recommendation.builder()
                .title("Increase Credit Score")
                .description("Improving credit score by 50 points could reduce churn probability by ~12%")
                .actionType("increase")
                .targetFeature("credit_score")
                .suggestedValue(750)
                .expectedImpact(-0.12)
                .priority("high")
                .build());

        recommendations.add(ExplainabilityDTO.Recommendation.builder()
                .title("Increase Product Usage")
                .description("Using 2+ additional products could reduce churn probability by ~8%")
                .actionType("increase")
                .targetFeature("num_products")
                .suggestedValue(4)
                .expectedImpact(-0.08)
                .priority("medium")
                .build());

        recommendations.add(ExplainabilityDTO.Recommendation.builder()
                .title("Maintain Account Activity")
                .description("Regular transactions help maintain customer engagement")
                .actionType("maintain")
                .targetFeature("last_activity_days")
                .suggestedValue(7)
                .expectedImpact(-0.05)
                .priority("medium")
                .build());

        return recommendations;
    }

    private String getRiskLevel(double probability) {
        if (probability >= 0.7) return "High Risk";
        if (probability >= 0.4) return "Medium Risk";
        return "Low Risk";
    }
}
